{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport missingno as msno\nfrom sklearn.model_selection import train_test_split # For splitting data into training and testing sets\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder  # For converting categorical data to numerical values\nfrom sklearn.metrics import accuracy_score  # For calculating the accuracy score of a classification model\nfrom sklearn.impute import KNNImputer  # For imputing missing values using K-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors classification model\nfrom xgboost import XGBClassifier  # XGBoost classification model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.over_sampling import ADASYN\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-18T00:08:37.307910Z","iopub.execute_input":"2024-08-18T00:08:37.308416Z","iopub.status.idle":"2024-08-18T00:08:37.337287Z","shell.execute_reply.started":"2024-08-18T00:08:37.308382Z","shell.execute_reply":"2024-08-18T00:08:37.335779Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# reading csv file \ndf_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nY = df_train[\"Survived\"]\nduplicate_values = df_train.duplicated()\n#print(f'Duplicate values = {duplicate_values}')\npassenger_ids = df_test['PassengerId'].copy()\n\nremove_col = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked','Survived']\nremove_col_val = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked']\nx = df_train.drop(labels=remove_col, axis=1)\nX_val = df_test.drop(labels=remove_col_val, axis=1)\n\nprint(x.describe(include = \"all\"))\ndf_test.head(20)\nmiss_values = df_test.isnull().sum()\nprint(miss_values)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T00:08:37.339668Z","iopub.execute_input":"2024-08-18T00:08:37.340511Z","iopub.status.idle":"2024-08-18T00:08:37.390361Z","shell.execute_reply.started":"2024-08-18T00:08:37.340465Z","shell.execute_reply":"2024-08-18T00:08:37.388875Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"            Pclass   Sex         Age       SibSp       Parch        Fare\ncount   891.000000   891  714.000000  891.000000  891.000000  891.000000\nunique         NaN     2         NaN         NaN         NaN         NaN\ntop            NaN  male         NaN         NaN         NaN         NaN\nfreq           NaN   577         NaN         NaN         NaN         NaN\nmean      2.308642   NaN   29.699118    0.523008    0.381594   32.204208\nstd       0.836071   NaN   14.526497    1.102743    0.806057   49.693429\nmin       1.000000   NaN    0.420000    0.000000    0.000000    0.000000\n25%       2.000000   NaN   20.125000    0.000000    0.000000    7.910400\n50%       3.000000   NaN   28.000000    0.000000    0.000000   14.454200\n75%       3.000000   NaN   38.000000    1.000000    0.000000   31.000000\nmax       3.000000   NaN   80.000000    8.000000    6.000000  512.329200\nPassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"x['FamilySize'] = x['SibSp'] + x['Parch'] + 1  # Add 1 to include the passenger themselves\nx['AgeGroup'] = pd.cut(x['Age'], bins=[0, 12, 18, 35, 60, 100], labels=['Child', 'Teenager', 'Adult', 'Middle-aged', 'Senior'])\n\nx.head()\n\nX_val['FamilySize'] = X_val['SibSp'] + X_val['Parch'] + 1  # Add 1 to include the passenger themselves\nX_val['AgeGroup'] = pd.cut(X_val['Age'], bins=[0, 12, 18, 35, 60, 100], labels=['Child', 'Teenager', 'Adult', 'Middle-aged', 'Senior'])\n\nX_val.head()\n\nprint(f'len of passenger_id = {len(passenger_ids)}')\nprint(f'len of predicted_classes = {len(X_val)}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T00:08:37.391956Z","iopub.execute_input":"2024-08-18T00:08:37.392471Z","iopub.status.idle":"2024-08-18T00:08:37.412353Z","shell.execute_reply.started":"2024-08-18T00:08:37.392436Z","shell.execute_reply":"2024-08-18T00:08:37.410938Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"len of passenger_id = 418\nlen of predicted_classes = 418\n","output_type":"stream"}]},{"cell_type":"code","source":"# Converting categorical values in 'Sex' to labels.\nle = LabelEncoder()\nx['Sex'] = le.fit_transform(x['Sex'])\nx['AgeGroup'] = le.fit_transform(x['AgeGroup'])\n\nX_val['Sex'] = le.fit_transform(X_val['Sex'])\nX_val['AgeGroup'] = le.fit_transform(X_val['AgeGroup'])\n\n\nprint(x.head())\n\n# Assuming original column names before imputation\ncolumns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare','FamilySize', 'AgeGroup']\n\n# Impute missing values\nimputer = KNNImputer(n_neighbors=5)\nx_df = imputer.fit_transform(x)\nX_val = imputer.fit_transform(X_val)\n# Convert the NumPy array back to a DataFrame with original column names\nx_df = pd.DataFrame(x_df, columns=columns)\nX_val = pd.DataFrame(X_val, columns=columns)\n\nx_df.head()\nX_val.head()\nprint(len(X_val))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T00:08:37.415618Z","iopub.execute_input":"2024-08-18T00:08:37.416127Z","iopub.status.idle":"2024-08-18T00:08:37.479184Z","shell.execute_reply.started":"2024-08-18T00:08:37.416081Z","shell.execute_reply":"2024-08-18T00:08:37.477593Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"   Pclass  Sex   Age  SibSp  Parch     Fare  FamilySize  AgeGroup\n0       3    1  22.0      1      0   7.2500           2         0\n1       1    0  38.0      1      0  71.2833           2         2\n2       3    0  26.0      0      0   7.9250           1         0\n3       1    0  35.0      1      0  53.1000           2         0\n4       3    1  35.0      0      0   8.0500           1         0\n418\n","output_type":"stream"}]},{"cell_type":"code","source":"sc = StandardScaler()\nx_df = sc.fit_transform(x_df)\nX_val = sc.fit_transform(X_val)\n\n# Convert the NumPy array back to a DataFrame with original column names\nx_df = pd.DataFrame(x_df, columns=columns)\nX_val = pd.DataFrame(X_val, columns=columns)\nprint(len(X_val))\nX_val.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T00:09:08.797598Z","iopub.execute_input":"2024-08-18T00:09:08.798040Z","iopub.status.idle":"2024-08-18T00:09:08.828042Z","shell.execute_reply.started":"2024-08-18T00:09:08.797996Z","shell.execute_reply":"2024-08-18T00:09:08.826501Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"418\n","output_type":"stream"},{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"     Pclass       Sex       Age     SibSp     Parch      Fare  FamilySize  \\\n0  0.873482  0.755929  0.379751 -0.499470 -0.400248 -0.497142   -0.553443   \n1  0.873482 -1.322876  1.291600  0.616992 -0.400248 -0.512006    0.105643   \n2 -0.315819  0.755929  2.385818 -0.499470 -0.400248 -0.463833   -0.553443   \n3  0.873482  0.755929 -0.167359 -0.499470 -0.400248 -0.482206   -0.553443   \n4  0.873482 -1.322876 -0.532098  0.616992  0.619896 -0.417228    0.764728   \n\n   AgeGroup  \n0 -0.964429  \n1  0.053579  \n2  0.562584  \n3 -0.964429  \n4 -0.964429  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>FamilySize</th>\n      <th>AgeGroup</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.873482</td>\n      <td>0.755929</td>\n      <td>0.379751</td>\n      <td>-0.499470</td>\n      <td>-0.400248</td>\n      <td>-0.497142</td>\n      <td>-0.553443</td>\n      <td>-0.964429</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.873482</td>\n      <td>-1.322876</td>\n      <td>1.291600</td>\n      <td>0.616992</td>\n      <td>-0.400248</td>\n      <td>-0.512006</td>\n      <td>0.105643</td>\n      <td>0.053579</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.315819</td>\n      <td>0.755929</td>\n      <td>2.385818</td>\n      <td>-0.499470</td>\n      <td>-0.400248</td>\n      <td>-0.463833</td>\n      <td>-0.553443</td>\n      <td>0.562584</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.873482</td>\n      <td>0.755929</td>\n      <td>-0.167359</td>\n      <td>-0.499470</td>\n      <td>-0.400248</td>\n      <td>-0.482206</td>\n      <td>-0.553443</td>\n      <td>-0.964429</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.873482</td>\n      <td>-1.322876</td>\n      <td>-0.532098</td>\n      <td>0.616992</td>\n      <td>0.619896</td>\n      <td>-0.417228</td>\n      <td>0.764728</td>\n      <td>-0.964429</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Split the data into training + validation and test sets\nX_train, X_test, y_train, y_test = train_test_split(x_df, Y, test_size=0.15, random_state=0)\n\n# Further split the training data into training and validation sets\n# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)  # 0.25 * 0.8 = 0.2\n\n# Convert the splits to PyTorch tensors\nX_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\nX_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n\n# Create PyTorch datasets\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T00:08:37.494300Z","iopub.execute_input":"2024-08-18T00:08:37.495866Z","iopub.status.idle":"2024-08-18T00:08:37.538861Z","shell.execute_reply.started":"2024-08-18T00:08:37.495811Z","shell.execute_reply":"2024-08-18T00:08:37.535772Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Set the seed\nset_seed(42)\n\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 256)    # Increase number of neurons\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 32)\n        self.fc5 = nn.Linear(32, 16)\n        self.fc6 = nn.Linear(16, 1)\n        \n        self.leaky_relu = nn.LeakyReLU()\n        self.dropout = nn.Dropout(0.5)\n        self.batchnorm1 = nn.BatchNorm1d(256)\n        self.batchnorm2 = nn.BatchNorm1d(128)\n        self.batchnorm3 = nn.BatchNorm1d(64)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        x = self.leaky_relu(self.batchnorm1(self.fc1(x)))\n        x = self.dropout(x)\n        x = self.leaky_relu(self.batchnorm2(self.fc2(x)))\n        x = self.dropout(x)\n        x = self.leaky_relu(self.batchnorm3(self.fc3(x)))\n        x = self.dropout(x)\n        x = self.leaky_relu(self.fc4(x))\n        x = self.dropout(x)\n        x = self.leaky_relu(self.fc5(x))\n        x = self.dropout(x)\n        x = self.sigmoid(self.fc6(x))\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T00:12:17.999940Z","iopub.execute_input":"2024-08-18T00:12:18.000418Z","iopub.status.idle":"2024-08-18T00:12:18.018761Z","shell.execute_reply.started":"2024-08-18T00:12:18.000385Z","shell.execute_reply":"2024-08-18T00:12:18.017267Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# Initialize the model\ninput_size = X_train_tensor.shape[1]  # Number of features\nmodel = SimpleNN(input_size)\n\n# Define the loss function and optimizer\ncriterion = nn.BCELoss()  # Binary Cross-Entropy Loss\noptimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T00:20:54.287974Z","iopub.execute_input":"2024-08-18T00:20:54.288422Z","iopub.status.idle":"2024-08-18T00:20:54.299733Z","shell.execute_reply.started":"2024-08-18T00:20:54.288387Z","shell.execute_reply":"2024-08-18T00:20:54.298080Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"# Training the model\nnum_epochs = 250\nmodel.train()\nbest_val_loss = float('inf')\nbest_model_weights = None\n\n\nfor epoch in range(num_epochs):\n    model.train()  # Ensure model is in training mode\n    running_loss = 0.0\n\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Accumulate the loss\n        running_loss += loss.item()\n    \n    # Calculate the average training loss for the epoch\n    avg_train_loss = running_loss / len(train_loader)\n    \n    # Evaluate the model on the validation set\n    model.eval()  # Ensure model is in evaluation mode\n    val_loss = 0.0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n    \n    avg_val_loss = val_loss / len(test_loader)\n    \n    # Save the model weights if the validation loss is the best we've seen so far\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_model_weights = model.state_dict()\n    \n    # Print the average losses for this epoch\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T00:20:54.631960Z","iopub.execute_input":"2024-08-18T00:20:54.632509Z","iopub.status.idle":"2024-08-18T00:21:42.422600Z","shell.execute_reply.started":"2024-08-18T00:20:54.632475Z","shell.execute_reply":"2024-08-18T00:21:42.421332Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"Epoch [1/250], Train Loss: 0.7102, Val Loss: 0.7064\nEpoch [2/250], Train Loss: 0.7126, Val Loss: 0.7028\nEpoch [3/250], Train Loss: 0.7159, Val Loss: 0.7014\nEpoch [4/250], Train Loss: 0.7148, Val Loss: 0.6982\nEpoch [5/250], Train Loss: 0.6948, Val Loss: 0.6960\nEpoch [6/250], Train Loss: 0.6957, Val Loss: 0.6935\nEpoch [7/250], Train Loss: 0.6949, Val Loss: 0.6921\nEpoch [8/250], Train Loss: 0.6872, Val Loss: 0.6898\nEpoch [9/250], Train Loss: 0.6927, Val Loss: 0.6879\nEpoch [10/250], Train Loss: 0.6932, Val Loss: 0.6864\nEpoch [11/250], Train Loss: 0.6860, Val Loss: 0.6836\nEpoch [12/250], Train Loss: 0.6823, Val Loss: 0.6825\nEpoch [13/250], Train Loss: 0.6839, Val Loss: 0.6797\nEpoch [14/250], Train Loss: 0.6780, Val Loss: 0.6764\nEpoch [15/250], Train Loss: 0.6775, Val Loss: 0.6736\nEpoch [16/250], Train Loss: 0.6770, Val Loss: 0.6743\nEpoch [17/250], Train Loss: 0.6730, Val Loss: 0.6711\nEpoch [18/250], Train Loss: 0.6708, Val Loss: 0.6699\nEpoch [19/250], Train Loss: 0.6800, Val Loss: 0.6690\nEpoch [20/250], Train Loss: 0.6686, Val Loss: 0.6687\nEpoch [21/250], Train Loss: 0.6649, Val Loss: 0.6649\nEpoch [22/250], Train Loss: 0.6620, Val Loss: 0.6640\nEpoch [23/250], Train Loss: 0.6614, Val Loss: 0.6607\nEpoch [24/250], Train Loss: 0.6680, Val Loss: 0.6599\nEpoch [25/250], Train Loss: 0.6688, Val Loss: 0.6575\nEpoch [26/250], Train Loss: 0.6640, Val Loss: 0.6538\nEpoch [27/250], Train Loss: 0.6594, Val Loss: 0.6543\nEpoch [28/250], Train Loss: 0.6651, Val Loss: 0.6563\nEpoch [29/250], Train Loss: 0.6604, Val Loss: 0.6526\nEpoch [30/250], Train Loss: 0.6463, Val Loss: 0.6492\nEpoch [31/250], Train Loss: 0.6532, Val Loss: 0.6508\nEpoch [32/250], Train Loss: 0.6566, Val Loss: 0.6471\nEpoch [33/250], Train Loss: 0.6553, Val Loss: 0.6475\nEpoch [34/250], Train Loss: 0.6448, Val Loss: 0.6487\nEpoch [35/250], Train Loss: 0.6529, Val Loss: 0.6458\nEpoch [36/250], Train Loss: 0.6505, Val Loss: 0.6432\nEpoch [37/250], Train Loss: 0.6449, Val Loss: 0.6404\nEpoch [38/250], Train Loss: 0.6486, Val Loss: 0.6356\nEpoch [39/250], Train Loss: 0.6444, Val Loss: 0.6396\nEpoch [40/250], Train Loss: 0.6474, Val Loss: 0.6354\nEpoch [41/250], Train Loss: 0.6356, Val Loss: 0.6316\nEpoch [42/250], Train Loss: 0.6545, Val Loss: 0.6322\nEpoch [43/250], Train Loss: 0.6509, Val Loss: 0.6363\nEpoch [44/250], Train Loss: 0.6466, Val Loss: 0.6320\nEpoch [45/250], Train Loss: 0.6469, Val Loss: 0.6328\nEpoch [46/250], Train Loss: 0.6381, Val Loss: 0.6359\nEpoch [47/250], Train Loss: 0.6379, Val Loss: 0.6304\nEpoch [48/250], Train Loss: 0.6343, Val Loss: 0.6321\nEpoch [49/250], Train Loss: 0.6337, Val Loss: 0.6267\nEpoch [50/250], Train Loss: 0.6383, Val Loss: 0.6262\nEpoch [51/250], Train Loss: 0.6313, Val Loss: 0.6247\nEpoch [52/250], Train Loss: 0.6331, Val Loss: 0.6263\nEpoch [53/250], Train Loss: 0.6288, Val Loss: 0.6268\nEpoch [54/250], Train Loss: 0.6296, Val Loss: 0.6222\nEpoch [55/250], Train Loss: 0.6500, Val Loss: 0.6238\nEpoch [56/250], Train Loss: 0.6300, Val Loss: 0.6218\nEpoch [57/250], Train Loss: 0.6271, Val Loss: 0.6247\nEpoch [58/250], Train Loss: 0.6294, Val Loss: 0.6284\nEpoch [59/250], Train Loss: 0.6304, Val Loss: 0.6196\nEpoch [60/250], Train Loss: 0.6416, Val Loss: 0.6171\nEpoch [61/250], Train Loss: 0.6272, Val Loss: 0.6189\nEpoch [62/250], Train Loss: 0.6186, Val Loss: 0.6134\nEpoch [63/250], Train Loss: 0.6163, Val Loss: 0.6131\nEpoch [64/250], Train Loss: 0.6204, Val Loss: 0.6117\nEpoch [65/250], Train Loss: 0.6144, Val Loss: 0.6139\nEpoch [66/250], Train Loss: 0.6337, Val Loss: 0.6180\nEpoch [67/250], Train Loss: 0.6158, Val Loss: 0.6132\nEpoch [68/250], Train Loss: 0.6252, Val Loss: 0.6120\nEpoch [69/250], Train Loss: 0.6122, Val Loss: 0.6143\nEpoch [70/250], Train Loss: 0.6334, Val Loss: 0.6100\nEpoch [71/250], Train Loss: 0.6285, Val Loss: 0.6214\nEpoch [72/250], Train Loss: 0.6179, Val Loss: 0.6182\nEpoch [73/250], Train Loss: 0.6237, Val Loss: 0.6069\nEpoch [74/250], Train Loss: 0.6144, Val Loss: 0.6138\nEpoch [75/250], Train Loss: 0.6057, Val Loss: 0.6017\nEpoch [76/250], Train Loss: 0.6153, Val Loss: 0.6008\nEpoch [77/250], Train Loss: 0.6124, Val Loss: 0.5982\nEpoch [78/250], Train Loss: 0.6060, Val Loss: 0.6019\nEpoch [79/250], Train Loss: 0.6098, Val Loss: 0.5900\nEpoch [80/250], Train Loss: 0.6257, Val Loss: 0.6089\nEpoch [81/250], Train Loss: 0.6138, Val Loss: 0.5974\nEpoch [82/250], Train Loss: 0.6165, Val Loss: 0.6056\nEpoch [83/250], Train Loss: 0.6022, Val Loss: 0.6038\nEpoch [84/250], Train Loss: 0.5967, Val Loss: 0.5956\nEpoch [85/250], Train Loss: 0.5901, Val Loss: 0.5947\nEpoch [86/250], Train Loss: 0.5949, Val Loss: 0.5944\nEpoch [87/250], Train Loss: 0.5957, Val Loss: 0.5890\nEpoch [88/250], Train Loss: 0.6015, Val Loss: 0.5893\nEpoch [89/250], Train Loss: 0.5887, Val Loss: 0.5834\nEpoch [90/250], Train Loss: 0.6148, Val Loss: 0.5772\nEpoch [91/250], Train Loss: 0.6039, Val Loss: 0.5832\nEpoch [92/250], Train Loss: 0.5916, Val Loss: 0.5855\nEpoch [93/250], Train Loss: 0.6147, Val Loss: 0.5847\nEpoch [94/250], Train Loss: 0.5912, Val Loss: 0.5871\nEpoch [95/250], Train Loss: 0.5793, Val Loss: 0.5888\nEpoch [96/250], Train Loss: 0.6214, Val Loss: 0.5763\nEpoch [97/250], Train Loss: 0.5778, Val Loss: 0.5889\nEpoch [98/250], Train Loss: 0.6137, Val Loss: 0.5737\nEpoch [99/250], Train Loss: 0.5910, Val Loss: 0.5547\nEpoch [100/250], Train Loss: 0.5817, Val Loss: 0.5663\nEpoch [101/250], Train Loss: 0.5908, Val Loss: 0.5755\nEpoch [102/250], Train Loss: 0.6009, Val Loss: 0.5795\nEpoch [103/250], Train Loss: 0.5826, Val Loss: 0.5695\nEpoch [104/250], Train Loss: 0.5830, Val Loss: 0.5594\nEpoch [105/250], Train Loss: 0.5814, Val Loss: 0.5509\nEpoch [106/250], Train Loss: 0.5759, Val Loss: 0.5571\nEpoch [107/250], Train Loss: 0.5841, Val Loss: 0.5637\nEpoch [108/250], Train Loss: 0.5753, Val Loss: 0.5614\nEpoch [109/250], Train Loss: 0.5867, Val Loss: 0.5481\nEpoch [110/250], Train Loss: 0.5689, Val Loss: 0.5448\nEpoch [111/250], Train Loss: 0.5683, Val Loss: 0.5413\nEpoch [112/250], Train Loss: 0.5677, Val Loss: 0.5497\nEpoch [113/250], Train Loss: 0.5702, Val Loss: 0.5488\nEpoch [114/250], Train Loss: 0.5628, Val Loss: 0.5444\nEpoch [115/250], Train Loss: 0.5409, Val Loss: 0.5382\nEpoch [116/250], Train Loss: 0.5563, Val Loss: 0.5452\nEpoch [117/250], Train Loss: 0.5675, Val Loss: 0.5613\nEpoch [118/250], Train Loss: 0.5545, Val Loss: 0.5406\nEpoch [119/250], Train Loss: 0.5708, Val Loss: 0.5387\nEpoch [120/250], Train Loss: 0.5603, Val Loss: 0.5306\nEpoch [121/250], Train Loss: 0.5759, Val Loss: 0.5417\nEpoch [122/250], Train Loss: 0.5758, Val Loss: 0.5388\nEpoch [123/250], Train Loss: 0.5420, Val Loss: 0.5370\nEpoch [124/250], Train Loss: 0.5538, Val Loss: 0.5287\nEpoch [125/250], Train Loss: 0.5504, Val Loss: 0.5158\nEpoch [126/250], Train Loss: 0.5470, Val Loss: 0.5297\nEpoch [127/250], Train Loss: 0.5451, Val Loss: 0.5244\nEpoch [128/250], Train Loss: 0.5498, Val Loss: 0.5165\nEpoch [129/250], Train Loss: 0.5495, Val Loss: 0.5254\nEpoch [130/250], Train Loss: 0.5314, Val Loss: 0.5286\nEpoch [131/250], Train Loss: 0.5479, Val Loss: 0.5119\nEpoch [132/250], Train Loss: 0.5684, Val Loss: 0.5130\nEpoch [133/250], Train Loss: 0.5680, Val Loss: 0.5160\nEpoch [134/250], Train Loss: 0.5403, Val Loss: 0.5196\nEpoch [135/250], Train Loss: 0.5504, Val Loss: 0.5180\nEpoch [136/250], Train Loss: 0.5653, Val Loss: 0.5059\nEpoch [137/250], Train Loss: 0.5457, Val Loss: 0.5106\nEpoch [138/250], Train Loss: 0.5620, Val Loss: 0.5040\nEpoch [139/250], Train Loss: 0.5478, Val Loss: 0.5086\nEpoch [140/250], Train Loss: 0.5460, Val Loss: 0.5181\nEpoch [141/250], Train Loss: 0.5471, Val Loss: 0.5082\nEpoch [142/250], Train Loss: 0.5330, Val Loss: 0.5101\nEpoch [143/250], Train Loss: 0.5530, Val Loss: 0.4983\nEpoch [144/250], Train Loss: 0.5328, Val Loss: 0.4989\nEpoch [145/250], Train Loss: 0.5254, Val Loss: 0.5004\nEpoch [146/250], Train Loss: 0.5385, Val Loss: 0.4998\nEpoch [147/250], Train Loss: 0.5738, Val Loss: 0.4955\nEpoch [148/250], Train Loss: 0.5486, Val Loss: 0.5043\nEpoch [149/250], Train Loss: 0.5337, Val Loss: 0.5001\nEpoch [150/250], Train Loss: 0.5380, Val Loss: 0.4980\nEpoch [151/250], Train Loss: 0.5384, Val Loss: 0.4959\nEpoch [152/250], Train Loss: 0.5377, Val Loss: 0.4995\nEpoch [153/250], Train Loss: 0.5447, Val Loss: 0.4995\nEpoch [154/250], Train Loss: 0.5441, Val Loss: 0.5103\nEpoch [155/250], Train Loss: 0.5285, Val Loss: 0.4966\nEpoch [156/250], Train Loss: 0.5242, Val Loss: 0.4938\nEpoch [157/250], Train Loss: 0.5350, Val Loss: 0.4847\nEpoch [158/250], Train Loss: 0.5247, Val Loss: 0.4912\nEpoch [159/250], Train Loss: 0.5350, Val Loss: 0.4958\nEpoch [160/250], Train Loss: 0.5329, Val Loss: 0.4925\nEpoch [161/250], Train Loss: 0.5177, Val Loss: 0.4888\nEpoch [162/250], Train Loss: 0.5302, Val Loss: 0.4875\nEpoch [163/250], Train Loss: 0.5106, Val Loss: 0.4826\nEpoch [164/250], Train Loss: 0.5190, Val Loss: 0.4768\nEpoch [165/250], Train Loss: 0.5389, Val Loss: 0.4756\nEpoch [166/250], Train Loss: 0.5360, Val Loss: 0.4849\nEpoch [167/250], Train Loss: 0.5133, Val Loss: 0.4789\nEpoch [168/250], Train Loss: 0.5177, Val Loss: 0.4796\nEpoch [169/250], Train Loss: 0.5181, Val Loss: 0.4782\nEpoch [170/250], Train Loss: 0.5186, Val Loss: 0.4765\nEpoch [171/250], Train Loss: 0.5411, Val Loss: 0.4703\nEpoch [172/250], Train Loss: 0.5408, Val Loss: 0.4755\nEpoch [173/250], Train Loss: 0.5129, Val Loss: 0.4775\nEpoch [174/250], Train Loss: 0.5256, Val Loss: 0.4799\nEpoch [175/250], Train Loss: 0.5332, Val Loss: 0.4755\nEpoch [176/250], Train Loss: 0.5246, Val Loss: 0.4696\nEpoch [177/250], Train Loss: 0.5165, Val Loss: 0.4734\nEpoch [178/250], Train Loss: 0.5339, Val Loss: 0.4701\nEpoch [179/250], Train Loss: 0.5278, Val Loss: 0.4663\nEpoch [180/250], Train Loss: 0.5377, Val Loss: 0.4723\nEpoch [181/250], Train Loss: 0.5248, Val Loss: 0.4733\nEpoch [182/250], Train Loss: 0.5305, Val Loss: 0.4754\nEpoch [183/250], Train Loss: 0.5529, Val Loss: 0.4710\nEpoch [184/250], Train Loss: 0.5154, Val Loss: 0.4716\nEpoch [185/250], Train Loss: 0.5222, Val Loss: 0.4627\nEpoch [186/250], Train Loss: 0.5219, Val Loss: 0.4661\nEpoch [187/250], Train Loss: 0.5121, Val Loss: 0.4617\nEpoch [188/250], Train Loss: 0.5371, Val Loss: 0.4761\nEpoch [189/250], Train Loss: 0.5034, Val Loss: 0.4746\nEpoch [190/250], Train Loss: 0.4944, Val Loss: 0.4665\nEpoch [191/250], Train Loss: 0.5221, Val Loss: 0.4580\nEpoch [192/250], Train Loss: 0.5055, Val Loss: 0.4704\nEpoch [193/250], Train Loss: 0.5119, Val Loss: 0.4589\nEpoch [194/250], Train Loss: 0.4919, Val Loss: 0.4677\nEpoch [195/250], Train Loss: 0.5142, Val Loss: 0.4556\nEpoch [196/250], Train Loss: 0.5093, Val Loss: 0.4585\nEpoch [197/250], Train Loss: 0.4981, Val Loss: 0.4605\nEpoch [198/250], Train Loss: 0.5307, Val Loss: 0.4560\nEpoch [199/250], Train Loss: 0.5013, Val Loss: 0.4635\nEpoch [200/250], Train Loss: 0.5082, Val Loss: 0.4632\nEpoch [201/250], Train Loss: 0.4958, Val Loss: 0.4630\nEpoch [202/250], Train Loss: 0.5252, Val Loss: 0.4571\nEpoch [203/250], Train Loss: 0.4954, Val Loss: 0.4635\nEpoch [204/250], Train Loss: 0.5116, Val Loss: 0.4536\nEpoch [205/250], Train Loss: 0.5272, Val Loss: 0.4533\nEpoch [206/250], Train Loss: 0.4982, Val Loss: 0.4691\nEpoch [207/250], Train Loss: 0.5144, Val Loss: 0.4613\nEpoch [208/250], Train Loss: 0.5056, Val Loss: 0.4475\nEpoch [209/250], Train Loss: 0.5140, Val Loss: 0.4527\nEpoch [210/250], Train Loss: 0.4977, Val Loss: 0.4569\nEpoch [211/250], Train Loss: 0.4762, Val Loss: 0.4535\nEpoch [212/250], Train Loss: 0.5056, Val Loss: 0.4515\nEpoch [213/250], Train Loss: 0.4998, Val Loss: 0.4512\nEpoch [214/250], Train Loss: 0.5087, Val Loss: 0.4527\nEpoch [215/250], Train Loss: 0.5062, Val Loss: 0.4513\nEpoch [216/250], Train Loss: 0.4945, Val Loss: 0.4431\nEpoch [217/250], Train Loss: 0.5042, Val Loss: 0.4556\nEpoch [218/250], Train Loss: 0.4981, Val Loss: 0.4595\nEpoch [219/250], Train Loss: 0.5039, Val Loss: 0.4572\nEpoch [220/250], Train Loss: 0.5037, Val Loss: 0.4551\nEpoch [221/250], Train Loss: 0.5044, Val Loss: 0.4576\nEpoch [222/250], Train Loss: 0.4997, Val Loss: 0.4549\nEpoch [223/250], Train Loss: 0.4850, Val Loss: 0.4517\nEpoch [224/250], Train Loss: 0.5083, Val Loss: 0.4461\nEpoch [225/250], Train Loss: 0.4995, Val Loss: 0.4484\nEpoch [226/250], Train Loss: 0.4895, Val Loss: 0.4572\nEpoch [227/250], Train Loss: 0.4975, Val Loss: 0.4545\nEpoch [228/250], Train Loss: 0.4950, Val Loss: 0.4547\nEpoch [229/250], Train Loss: 0.5128, Val Loss: 0.4544\nEpoch [230/250], Train Loss: 0.4930, Val Loss: 0.4525\nEpoch [231/250], Train Loss: 0.4905, Val Loss: 0.4523\nEpoch [232/250], Train Loss: 0.5064, Val Loss: 0.4477\nEpoch [233/250], Train Loss: 0.4945, Val Loss: 0.4509\nEpoch [234/250], Train Loss: 0.4901, Val Loss: 0.4589\nEpoch [235/250], Train Loss: 0.4904, Val Loss: 0.4526\nEpoch [236/250], Train Loss: 0.4759, Val Loss: 0.4539\nEpoch [237/250], Train Loss: 0.4915, Val Loss: 0.4566\nEpoch [238/250], Train Loss: 0.5105, Val Loss: 0.4547\nEpoch [239/250], Train Loss: 0.4960, Val Loss: 0.4611\nEpoch [240/250], Train Loss: 0.4929, Val Loss: 0.4538\nEpoch [241/250], Train Loss: 0.5048, Val Loss: 0.4575\nEpoch [242/250], Train Loss: 0.4834, Val Loss: 0.4477\nEpoch [243/250], Train Loss: 0.5048, Val Loss: 0.4590\nEpoch [244/250], Train Loss: 0.4814, Val Loss: 0.4529\nEpoch [245/250], Train Loss: 0.4811, Val Loss: 0.4451\nEpoch [246/250], Train Loss: 0.4885, Val Loss: 0.4548\nEpoch [247/250], Train Loss: 0.4998, Val Loss: 0.4571\nEpoch [248/250], Train Loss: 0.4711, Val Loss: 0.4441\nEpoch [249/250], Train Loss: 0.4891, Val Loss: 0.4397\nEpoch [250/250], Train Loss: 0.4985, Val Loss: 0.4404\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_state_dict(best_model_weights)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Evaluate the best model on the validation (or test) set\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:  # You can also use val_loader if you want to evaluate on the validation set\n        outputs = model(inputs)\n        predicted = (outputs > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n# Calculate accuracy\naccuracy = 100 * correct / total\nprint(f'Best Model Accuracy: {accuracy:.2f}%')\n\ntorch.save(best_model_weights, '/kaggle/working/best_model_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2024-08-18T00:21:46.314116Z","iopub.execute_input":"2024-08-18T00:21:46.314582Z","iopub.status.idle":"2024-08-18T00:21:46.416064Z","shell.execute_reply.started":"2024-08-18T00:21:46.314548Z","shell.execute_reply":"2024-08-18T00:21:46.414655Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stdout","text":"Best Model Accuracy: 82.09%\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\n\n\nmodel.load_state_dict(torch.load('/kaggle/working/best_model_weights.pth'))\n\n# Run inference\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X_val_tensor)\n    predicted_classes = (predictions > 0.5).int().view(-1)  # Convert to binary classes\n    \n# Assuming X_val is already preprocessed and is a tensor ready for inference\nprint(f'len of passenger_id = {len(passenger_ids)}')\nprint(f'len of predicted_classes = {len(predicted_classes)}')\n\n# Create a DataFrame with PassengerId and Survived\noutput_df = pd.DataFrame({\n    'PassengerId': passenger_ids,  # Use the stored PassengerId\n    'Survived': predicted_classes.numpy()\n})\n\n# Save to CSV\noutput_df.to_csv('/kaggle/working/submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T00:21:51.636291Z","iopub.execute_input":"2024-08-18T00:21:51.636728Z","iopub.status.idle":"2024-08-18T00:21:51.657863Z","shell.execute_reply.started":"2024-08-18T00:21:51.636694Z","shell.execute_reply":"2024-08-18T00:21:51.655884Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"len of passenger_id = 418\nlen of predicted_classes = 418\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}